{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VYpOqeQv97rh"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "from matplotlib import colors\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5z_GoVK-DJK",
        "outputId": "60d9d7f7-2ef1-4e98-be3f-294023e68bb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/siamese-registration')"
      ],
      "metadata": {
        "id": "xl4h8SEh-FeH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models import *\n",
        "from utils import get_transformation_matrix"
      ],
      "metadata": {
        "id": "K2hyMJKTAgkW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"01/model-9.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = siamese_resnet18(1, 7)\n",
        "model.load_state_dict(torch.load(os.path.join(\"/content/drive/MyDrive/outputs/models\", checkpoint), map_location=device))\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "print(f\"Running on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXmebxauADkT",
        "outputId": "b7cea30c-2594-4c4e-b17c-a98e726fac81"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/data/Study_02_00008_02_R.avi\")\n",
        "# Find the number of frames\n",
        "video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
        "count = 0\n",
        "# Start converting the video\n",
        "image_list = []\n",
        "image_list_warped = []\n",
        "time_start = time.time()\n",
        "with tqdm(total=video_length, unit=' frames') as pbar:\n",
        "    while cap.isOpened():\n",
        "        # Extract the frame\n",
        "\n",
        "        ret, frame = cap.read()\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        if not ret:\n",
        "            continue\n",
        "        if len(image_list) == 0:\n",
        "            image_list.append(frame)\n",
        "            image_list_warped.append(frame)\n",
        "        else:\n",
        "            frame0 = image_list_warped[count-1]\n",
        "            frame1 = frame\n",
        "\n",
        "            img0 = torch.unsqueeze(transforms.functional.to_tensor(frame0), 1).to(device)\n",
        "            img1 = torch.unsqueeze(transforms.functional.to_tensor(frame1), 1).to(device)\n",
        "\n",
        "            start = time.perf_counter()\n",
        "            outputs = model(img0, img1)\n",
        "            end = time.perf_counter()\n",
        "            #print(f\"{count}, time: {end - start}\")\n",
        "\n",
        "            rows, cols = frame0.shape\n",
        "            center = (cols//2, rows//2)\n",
        "            tx, ty, sx, sy, shx, shy, q = outputs.detach().cpu().numpy().reshape(-1).tolist()\n",
        "            matrix = get_transformation_matrix(center, tx, ty, sx, sy, shx, shy, q)\n",
        "            matrix_opencv = np.float32(matrix.flatten()[:6].reshape(2, 3))\n",
        "            inverse_matrix = cv2.invertAffineTransform(matrix_opencv)\n",
        "\n",
        "            frame1_warped = cv2.warpAffine(frame1, inverse_matrix, (cols, rows))\n",
        "\n",
        "            image_list.append(frame)\n",
        "            image_list_warped.append(frame1_warped)\n",
        "\n",
        "        \n",
        "        count = count + 1\n",
        "        pbar.update(1)\n",
        "        # If there are no more frames left\n",
        "        if (count > (video_length-1)):\n",
        "            # Log the time again\n",
        "            time_end = time.time()\n",
        "            # Release the feed\n",
        "            cap.release()\n",
        "            # Print stats\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp7HamGiAhFk",
        "outputId": "2577c0c4-12d3-4e92-ce9f-1ff46c5f9a41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of frames:  253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 253/253 [00:14<00:00, 17.43 frames/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = np.concatenate((image_list[0], image_list_warped[0]), axis = 1)\n",
        "\n",
        "# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\n",
        "out = cv2.VideoWriter('/content/drive/MyDrive/outputs/video/Study_02_00008_02_R.mp4', cv2.VideoWriter_fourcc(*'XVID'), 15, (img.shape[1], img.shape[0]), 0)\n",
        "\n",
        "for image, image_warped in zip(image_list, image_list_warped):\n",
        "    img = np.concatenate((image, image_warped), axis = 1)\n",
        "    I = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "    out.write(I)\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "out.release()"
      ],
      "metadata": {
        "id": "x4iILq4yGMY-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qm2Q7qzGj3i",
        "outputId": "6e777bda-f372-4f3d-89a3-dea36b896523"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(770, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ULczAJX-MPxw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}